{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe9bc8d5-b2cb-4910-97e3-880f2009ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "from ipykernel import get_connection_file\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.retry import DEFAULT_RETRY\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00cc83d9-65ff-4cdb-83d1-553709fb4e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive examples: auto_ml_video_classifier/Talking_to_camera - Talking_to_camera.csv\n",
      "negative examples: auto_ml_video_classifier/not_talking_to_camera - not_talking_to_camera.csv\n",
      "input path: gs://jkwng-vi-api-files/input\n",
      "datasets path: gs://jkwng-vi-api-files/datasets\n",
      "eval path : gs://jkwng-vi-api-files/eval\n",
      "local dataset dir: /home/jupyter/auto_ml_video_classifier/datasets/\n",
      "local eval dir: /home/jupyter/auto_ml_video_classifier/eval/\n"
     ]
    }
   ],
   "source": [
    "gcs_bucket = \"jkwng-vi-api-files\"\n",
    "video_bucket_prefix = \"input\"\n",
    "dataset_bucket_prefix = \"datasets\"\n",
    "dataset_eval_prefix = \"eval\"\n",
    "local_datasets_prefix = \"datasets\"\n",
    "local_eval_prefix = \"eval\"\n",
    "output_prediction_prefix = \"prediction\"\n",
    "\n",
    "neg_input_csv_file = 'auto_ml_video_classifier/not_talking_to_camera - not_talking_to_camera.csv'\n",
    "pos_input_csv_file = 'auto_ml_video_classifier/Talking_to_camera - Talking_to_camera.csv'\n",
    "\n",
    "print(f\"positive examples: {pos_input_csv_file}\")\n",
    "print(f\"negative examples: {neg_input_csv_file}\")\n",
    "print(f\"input path: gs://{gcs_bucket}/{video_bucket_prefix}\")\n",
    "print(f\"datasets path: gs://{gcs_bucket}/{dataset_bucket_prefix}\")\n",
    "print(f\"eval path : gs://{gcs_bucket}/{dataset_eval_prefix}\")\n",
    "\n",
    "current_workdir = os.getcwd()\n",
    "\n",
    "print(f\"local dataset dir: {current_workdir}/{local_datasets_prefix}/\")\n",
    "print(f\"local eval dir: {current_workdir}/{local_eval_prefix}/\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(f\"{current_workdir}/{local_datasets_prefix}\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(f\"{current_workdir}/{local_eval_prefix}\")\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ef5b9e-9499-4c53-855f-9dd9298c9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(gcs_bucket)\n",
    "\n",
    "url_downloader = urllib.request.URLopener()\n",
    "\n",
    "#blob = bucket.blob(\"test_blob\")\n",
    "\n",
    "# Mode can be specified as wb/rb for bytes mode.\n",
    "# See: https://docs.python.org/3/library/io.html\n",
    "#with blob.open(\"w\") as f:\n",
    "#    f.write(\"Hello world\")\n",
    "#\n",
    "#with blob.open(\"r\") as f:\n",
    "#    print(f.read())\n",
    "\n",
    "\n",
    "\n",
    "def num_rows(filename):\n",
    "    with open(filename) as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile, quotechar='\"')\n",
    "        num_rows = len(list(csvreader))\n",
    "        return num_rows\n",
    "\n",
    "def write_blob_to_gcs(url, gcs_path):\n",
    "    blob_name = url.split('/')[-1]\n",
    "    #print(f\"{url}, basename: {blob_name}, gcs_path: {gcs_path}\")\n",
    "    blob = bucket.blob(f\"{video_bucket_prefix}/{blob_name}\")\n",
    "    if blob.exists():\n",
    "        # print(f\"blob exists\")\n",
    "        return\n",
    "\n",
    "    # print(f\"blob does not exist, downloading\")\n",
    "    url_downloader.retrieve(url, f\"/tmp/{blob_name}\")\n",
    "    blob.upload_from_filename(f\"/tmp/{blob_name}\", retry=DEFAULT_RETRY)\n",
    "    # print(f\"blob uploaded\")\n",
    "    os.remove(f\"/tmp/{blob_name}\")\n",
    "\n",
    "def upload_local_file_to_gcs(filename, gcs_path):\n",
    "    obj = bucket.blob(gcs_path)\n",
    "    obj.upload_from_filename(filename, retry=DEFAULT_RETRY)\n",
    "    #os.remove(filename)\n",
    "    print(f\"wrote local file {filename} to GCS: {gcs_path}\")\n",
    "\n",
    "def write_training_dataset_record(filename, gcs_path, label, use):\n",
    "\n",
    "    # generate the jsonl\n",
    "#        {\n",
    "#            \"videoGcsUri\": \"gs://bucket/filename.ext\",\n",
    "#            \"timeSegmentAnnotations\": [{\n",
    "#                \"displayName\": \"LABEL\",\n",
    "#                \"startTime\": \"start_time_of_segment\",\n",
    "#                \"endTime\": \"end_time_of_segment\"\n",
    "#            }],\n",
    "#            \"dataItemResourceLabels\": {\n",
    "#                \"aiplatform.googleapis.com/ml_use\": \"train|test\"\n",
    "#            }\n",
    "#        }\n",
    "\n",
    "    obj = {\n",
    "        \"videoGcsUri\": gcs_path,\n",
    "        \"timeSegmentAnnotations\": [{\n",
    "            \"displayName\": label,\n",
    "            \"startTime\": \"0s\",\n",
    "            \"endTime\": \"Infinity\",\n",
    "        }],\n",
    "    }\n",
    "\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"{json.dumps(obj)}\\n\")\n",
    "        f.close()\n",
    "\n",
    "def write_eval_dataset_record(gcs_path, filename):\n",
    "    # write a file we can use to start a batch training job\n",
    "    obj2 = {\n",
    "        'content': gcs_path, \n",
    "        'mimeType': 'video/mp4', \n",
    "        'timeSegmentStart': '0s', \n",
    "        'timeSegmentEnd': 'Infinity'\n",
    "    }\n",
    "\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"{json.dumps(obj2)}\\n\")\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df4595-c1d7-46ea-a9a0-2403c19e80bb",
   "metadata": {},
   "source": [
    "1. read csv files\n",
    "2. download and upload the files to input bucket if they don't exist\n",
    "3. create jsonl file with positive and negative examples\n",
    "4. write json files to GCS\n",
    "5. write eval datasets to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "134fed26-aaf1-4f88-a935-81e964f0447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows talking_to_camera: 388\n",
      "number of rows not_talking_to_camera: 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_neg = num_rows(neg_input_csv_file)\n",
    "num_pos = num_rows(pos_input_csv_file)\n",
    "\n",
    "print(f\"number of rows talking_to_camera: {num_pos}\")\n",
    "print(f\"number of rows not_talking_to_camera: {num_neg}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f2cb6-212e-4503-9195-60156e8810e8",
   "metadata": {},
   "source": [
    "- TODO: use VI API shot detection to split up files by shot\n",
    "- TODO: if we can classify talking/not_talking by shot, it may improve model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad79906c-eb69-4063-bf88-5babb47bc9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_label_dataset = {\n",
    "    \"train\": f\"{local_datasets_prefix}/vi_api_multi_label_train.jsonl\",\n",
    "    \"test\": f\"{local_datasets_prefix}/vi_api_multi_label_test.jsonl\",\n",
    "}\n",
    "\n",
    "\n",
    "ttc_dataset = {\n",
    "    \"pos\": f\"{local_datasets_prefix}/vi_api_out_ttc_pos.jsonl\",\n",
    "    \"neg\": f\"{local_datasets_prefix}/vi_api_out_ttc_neg.jsonl\"\n",
    "}\n",
    "ttc_eval = {\n",
    "    \"pos\": f\"{local_eval_prefix}/vi_api_eval_ttc_pos.jsonl\",\n",
    "    \"neg\": f\"{local_eval_prefix}/vi_api_eval_ttc_neg.jsonl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f7ae8129-679c-44d4-a0f4-71971cde737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa3b82-6c12-4d5f-adf8-62d8f4f31380",
   "metadata": {},
   "source": [
    "- split the dataset into the following:\n",
    "  - train 60%\n",
    "  - test 20%\n",
    "  - validate 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e5a78b58-289b-47ac-b764-21cd8e5ecc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset everything\n",
    "all_content_types = defaultdict(list)\n",
    "\n",
    "for filename in os.listdir(local_datasets_prefix):\n",
    "    if not filename.endswith(\"jsonl\"):\n",
    "        continue\n",
    "        \n",
    "    print(f\"removing: {local_datasets_prefix}/{filename}\")\n",
    "    os.remove(f\"{local_datasets_prefix}/{filename}\")\n",
    "    \n",
    "for filename in os.listdir(local_eval_prefix):\n",
    "    if not filename.endswith(\"jsonl\"):\n",
    "        continue\n",
    "    \n",
    "    print(f\"removing: {local_eval_prefix}/{filename}\")\n",
    "    os.remove(f\"{local_eval_prefix}/{filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "90396e0b-913f-4e99-a87c-d32e813dc088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 500 negative records; train: 400, eval: 100\n"
     ]
    }
   ],
   "source": [
    "with open(neg_input_csv_file) as csvfile:\n",
    "    csvreader = csv.DictReader(csvfile, quotechar='\"')\n",
    "    rowcount = 0\n",
    "    train_rowcount = 0\n",
    "    eval_rowcount = 0\n",
    "    \n",
    "    for row in csvreader:\n",
    "        # write a line to the training output:\n",
    "        # train 60%\n",
    "        # test 20%\n",
    "        # validate 20%\n",
    "        if rowcount < num_neg * 0.6:\n",
    "            use = \"train\"\n",
    "        elif rowcount < num_neg * 0.8:\n",
    "            use = \"test\"\n",
    "        else:\n",
    "            use = \"evaluate\"\n",
    "\n",
    "        # get the basename\n",
    "        blob_name = row['preview_url'].split('/')[-1]\n",
    "        gcs_path = f\"gs://{gcs_bucket}/{video_bucket_prefix}/{blob_name}\"\n",
    "        \n",
    "        # get the blob\n",
    "        write_blob_to_gcs(row['preview_url'], gcs_path)\n",
    "\n",
    "        content_types = set(map(lambda x: x.replace(\"{\", \"\").replace(\"}\", \"\").replace(\"\\\"\", \"\").replace(\" \", \"_\").lower(), row['content_types'].split(\",\")))\n",
    "        content_types.discard(\"\")\n",
    "        for content_type in content_types:\n",
    "            all_content_types[content_type].append(gcs_path)\n",
    "\n",
    "        if use == \"evaluate\":\n",
    "            eval_rowcount += 1\n",
    "            write_eval_dataset_record(gcs_path, ttc_eval[\"neg\"])\n",
    "        else:\n",
    "            train_rowcount += 1\n",
    "            write_training_dataset_record(ttc_dataset[\"neg\"], gcs_path, \"not_talking_to_camera\", use)\n",
    "\n",
    "        rowcount += 1\n",
    "        \n",
    "print(f\"total {rowcount} negative records; train: {train_rowcount}, eval: {eval_rowcount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "11b048b8-ecc2-4c2c-9afd-cef7a0c042f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 388 positive records; train: 311, eval: 77\n"
     ]
    }
   ],
   "source": [
    "with open(pos_input_csv_file ) as csvfile:\n",
    "    csvreader = csv.DictReader(csvfile, quotechar='\"')\n",
    "\n",
    "    rowcount = 0\n",
    "    train_rowcount = 0\n",
    "    eval_rowcount = 0\n",
    "    \n",
    "    for row in csvreader:\n",
    "        if rowcount < num_pos * 0.6:\n",
    "            use = \"train\"\n",
    "        elif rowcount < num_pos * 0.8:\n",
    "            use = \"test\"\n",
    "        else:\n",
    "            use = \"evaluate\"\n",
    "\n",
    "        # get the basename\n",
    "        blob_name = row['preview_url'].split('/')[-1]\n",
    "        gcs_path = f\"gs://{gcs_bucket}/{video_bucket_prefix}/{blob_name}\"\n",
    "        \n",
    "        # get the blob\n",
    "        write_blob_to_gcs(row['preview_url'], gcs_path)\n",
    "\n",
    "        content_types = set(map(lambda x: x.replace(\"{\", \"\").replace(\"}\", \"\").replace(\"\\\"\", \"\").replace(\" \", \"_\").lower(), row['content_types'].split(\",\")))\n",
    "        content_types.discard(\"\")\n",
    "        for content_type in content_types:\n",
    "            all_content_types[content_type].append(gcs_path)\n",
    "\n",
    "        # write a line to the training output:\n",
    "        # train 60%\n",
    "        # test 20%\n",
    "        # validate 20%\n",
    "\n",
    "        if use == \"evaluate\":\n",
    "            eval_rowcount += 1\n",
    "            write_eval_dataset_record(gcs_path, ttc_eval[\"pos\"])\n",
    "        else:\n",
    "            train_rowcount += 1\n",
    "            write_training_dataset_record(ttc_dataset[\"pos\"], gcs_path, \"talking_to_camera\", use)\n",
    "\n",
    "        rowcount += 1\n",
    "\n",
    "print(f\"total {rowcount} positive records; train: {train_rowcount}, eval: {eval_rowcount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5a1861e0-572e-45b8-8793-ff886540ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_type: recipe, total: 50,  train: 40, eval: 10\n",
      "content_type: voiceover, total: 274,  train: 220, eval: 54\n",
      "content_type: texture, total: 77,  train: 62, eval: 15\n",
      "content_type: educational, total: 266,  train: 213, eval: 53\n",
      "content_type: product_focus, total: 557,  train: 446, eval: 111\n",
      "content_type: aesthetic, total: 292,  train: 234, eval: 58\n",
      "content_type: trend, total: 158,  train: 127, eval: 31\n",
      "content_type: lifestyle, total: 434,  train: 348, eval: 86\n",
      "content_type: unboxing, total: 115,  train: 92, eval: 23\n",
      "content_type: asmr, total: 26,  train: 21, eval: 5\n",
      "content_type: humor, total: 74,  train: 60, eval: 14\n",
      "content_type: talking_to_camera, total: 388,  train: 311, eval: 77\n",
      "total records: train: 2174, eval: 537\n"
     ]
    }
   ],
   "source": [
    "#for content_type, itemlist in all_content_types.items():\n",
    "#    print(f\"{content_type}: {len(itemlist)}\")\n",
    "\n",
    "total_records_train = 0\n",
    "total_records_eval = 0\n",
    "\n",
    "for content_type, itemlist in all_content_types.items():\n",
    "    train_rowcount = 0\n",
    "    eval_rowcount = 0\n",
    "    \n",
    "    rowcount = 0\n",
    "    for row in itemlist:\n",
    "        if rowcount < len(itemlist) * 0.6:\n",
    "            use = \"train\"\n",
    "        elif rowcount < len(itemlist) * 0.8:\n",
    "            use = \"test\"\n",
    "        else:\n",
    "            use = \"evaluate\"\n",
    "        \n",
    "        gcs_path = row\n",
    "        \n",
    "        if use == \"evaluate\":\n",
    "            eval_rowcount += 1\n",
    "            write_eval_dataset_record(gcs_path, f\"{local_eval_prefix}/vi_api_eval_{content_type}.jsonl\")\n",
    "        else:\n",
    "            train_rowcount += 1\n",
    "            write_training_dataset_record(multi_label_dataset[use], gcs_path, content_type, use)\n",
    "        \n",
    "        rowcount += 1\n",
    "        \n",
    "        \n",
    "    print(f\"content_type: {content_type}, total: {rowcount},  train: {train_rowcount}, eval: {eval_rowcount}\")\n",
    "    \n",
    "    total_records_train += train_rowcount\n",
    "    total_records_eval += eval_rowcount\n",
    "    \n",
    "print(f\"total records: train: {total_records_train}, eval: {total_records_eval}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1fbd70d8-ee28-4ca9-a04a-2dc812c6603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote local file datasets/vi_api_multi_label_train.jsonl to GCS: datasets/vi_api_multi_label_train.jsonl\n",
      "wrote local file datasets/vi_api_multi_label_test.jsonl to GCS: datasets/vi_api_multi_label_test.jsonl\n",
      "wrote local file eval/vi_api_eval_recipe.jsonl to GCS: eval/vi_api_eval_recipe.jsonl\n",
      "wrote local file eval/vi_api_eval_voiceover.jsonl to GCS: eval/vi_api_eval_voiceover.jsonl\n",
      "wrote local file eval/vi_api_eval_texture.jsonl to GCS: eval/vi_api_eval_texture.jsonl\n",
      "wrote local file eval/vi_api_eval_educational.jsonl to GCS: eval/vi_api_eval_educational.jsonl\n",
      "wrote local file eval/vi_api_eval_product_focus.jsonl to GCS: eval/vi_api_eval_product_focus.jsonl\n",
      "wrote local file eval/vi_api_eval_aesthetic.jsonl to GCS: eval/vi_api_eval_aesthetic.jsonl\n",
      "wrote local file eval/vi_api_eval_trend.jsonl to GCS: eval/vi_api_eval_trend.jsonl\n",
      "wrote local file eval/vi_api_eval_lifestyle.jsonl to GCS: eval/vi_api_eval_lifestyle.jsonl\n",
      "wrote local file eval/vi_api_eval_unboxing.jsonl to GCS: eval/vi_api_eval_unboxing.jsonl\n",
      "wrote local file eval/vi_api_eval_asmr.jsonl to GCS: eval/vi_api_eval_asmr.jsonl\n",
      "wrote local file eval/vi_api_eval_humor.jsonl to GCS: eval/vi_api_eval_humor.jsonl\n",
      "wrote local file eval/vi_api_eval_talking_to_camera.jsonl to GCS: eval/vi_api_eval_talking_to_camera.jsonl\n",
      "wrote local file datasets/vi_api_out_ttc_pos.jsonl to GCS: datasets/vi_api_out_ttc_pos.jsonl\n",
      "wrote local file datasets/vi_api_out_ttc_neg.jsonl to GCS: datasets/vi_api_out_ttc_neg.jsonl\n",
      "wrote local file eval/vi_api_eval_ttc_pos.jsonl to GCS: eval/eval/vi_api_eval_ttc_pos.jsonl\n",
      "wrote local file eval/vi_api_eval_ttc_neg.jsonl to GCS: eval/eval/vi_api_eval_ttc_neg.jsonl\n"
     ]
    }
   ],
   "source": [
    "for dataset in multi_label_dataset.values():\n",
    "    upload_local_file_to_gcs(dataset, f\"{dataset_bucket_prefix}/{os.path.basename(dataset)}\")\n",
    "\n",
    "#print(f\"all content types: {all_content_types}\")\n",
    "for content_type in all_content_types:\n",
    "    upload_local_file_to_gcs(f\"{local_eval_prefix}/vi_api_eval_{content_type}.jsonl\", f\"{dataset_eval_prefix}/vi_api_eval_{content_type}.jsonl\")\n",
    "\n",
    "for dataset in ttc_dataset.values():\n",
    "    upload_local_file_to_gcs(dataset, f\"{dataset_bucket_prefix}/{os.path.basename(dataset)}\")\n",
    "\n",
    "for file in ttc_eval.values():\n",
    "    upload_local_file_to_gcs(file, f\"{dataset_eval_prefix}/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e0fce7-6d01-43bb-9500-aa4feefd273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://{gcs_bucket}/staging\"  # @param {type:\"string\"}\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init()\n",
    "#aiplatform_v1.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f7a29c78-87f3-4482-afe1-f35a097c439f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/205512073711/locations/us-central1/datasets/5040254760213544960\n"
     ]
    }
   ],
   "source": [
    "# create the talking_to_camera dataset\n",
    "\n",
    "ttc_dataset = aiplatform.VideoDataset.create(\n",
    "    display_name=\"talking_to_camera_v2\",\n",
    "    gcs_source=[\n",
    "        f\"gs://{gcs_bucket}/{dataset_bucket_prefix}/{os.path.basename(ttc_dataset['pos'])}\",\n",
    "        f\"gs://{gcs_bucket}/{dataset_bucket_prefix}/{os.path.basename(ttc_dataset['neg'])}\",\n",
    "    ],\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.video.classification,\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "08e7e747-10c8-4bcb-aab8-6c4a941fa483",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'resource_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m     multi_label_gcs_sources\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgcs_bucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_bucket_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m multi_label_dataset \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mVideoDataset\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      8\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classifier_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     gcs_source\u001b[38;5;241m=\u001b[39mmulti_label_gcs_sources,\n\u001b[1;32m     10\u001b[0m     import_schema_uri\u001b[38;5;241m=\u001b[39maiplatform\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mioformat\u001b[38;5;241m.\u001b[39mvideo\u001b[38;5;241m.\u001b[39mclassification,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_name\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'resource_name'"
     ]
    }
   ],
   "source": [
    "# create multi-label training dataset\n",
    "\n",
    "multi_label_gcs_sources = []\n",
    "for dataset in multi_label_dataset.values():\n",
    "    multi_label_gcs_sources.append(f\"gs://{gcs_bucket}/{dataset_bucket_prefix}/{os.path.basename(dataset)}\")\n",
    "\n",
    "multi_label_dataset = aiplatform.VideoDataset.create(\n",
    "    display_name=\"multi_label_classifier_v2\",\n",
    "    gcs_source=multi_label_gcs_sources,\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.video.classification,\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "08a45f16-15d1-40c7-aa4e-02e63cc2034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.training_jobs.AutoMLVideoTrainingJob object at 0x7fa2e6899f00>\n"
     ]
    }
   ],
   "source": [
    "# create the training job for ttc\n",
    "\n",
    "ttc_job = aiplatform.AutoMLVideoTrainingJob(\n",
    "    display_name=\"ttc_training_job\",\n",
    "    prediction_type=\"classification\",\n",
    ")\n",
    "\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1885ee0c-d8ba-4564-8e00-83c612151544",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ttc_job.run(\n",
    "    dataset=ttc_dataset,\n",
    "    model_display_name=\"talking_to_camera\",\n",
    "    training_fraction_split=0.8,\n",
    "    test_fraction_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0435ee8-00ba-4abf-bb6c-1ba0b65c76ba",
   "metadata": {},
   "source": [
    "Run a batch prediction on one record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a9f903c-5515-47ec-a6e5-575e4333a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.models.Model object at 0x7faf9e53dba0> \n",
      "resource name: projects/205512073711/locations/us-central1/models/talking_to_camera_v1\n",
      "0\n",
      "{'content': 'gs://jkwng-vi-api-files/input/9cc7590c-fc07-48ae-b3aa-5c67de3dc95b.mp4', 'mimeType': 'video/mp4', 'timeSegmentStart': '0s', 'timeSegmentEnd': 'Infinity'}\n"
     ]
    }
   ],
   "source": [
    "# create a batch prediction\n",
    "ttc_model = aiplatform.Model(\"talking_to_camera_v1\")\n",
    "print(f\"{ttc_model}\")\n",
    "\n",
    "# load the eval file with a positive prediction\n",
    "eval_file = bucket.blob(f\"{dataset_eval_prefix}/{ttc_eval['pos']}\")\n",
    "blobreader = storage.fileio.BlobReader(eval_file)\n",
    "print(blobreader.tell())\n",
    "\n",
    "line = blobreader.readline().decode('utf-8')\n",
    "obj = json.loads(line)\n",
    "print(f\"{obj}\")\n",
    "\n",
    "prediction_input = bucket.blob(f\"prediction/test_prediction.jsonl\")\n",
    "blobwriter = storage.fileio.BlobWriter(prediction_input)\n",
    "blobwriter.write(json.dumps(obj).encode())\n",
    "blobwriter.close()\n",
    "\n",
    "\n",
    "job = ttc_model.batch_predict(\n",
    "    job_display_name=\"talking_to_camera_pos\",\n",
    "    gcs_source=f\"gs://{gcs_bucket}/prediction/test_prediction.jsonl\",\n",
    "    gcs_destination_prefix=f\"gs://{gcs_bucket}/{output_prediction_prefix}\",\n",
    "    sync=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91278aa4-2d96-456f-a061-90b68e605e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
